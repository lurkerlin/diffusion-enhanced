{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "syn_data_size:12240\n",
      "train_data_size:12240\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, datasets, transforms\n",
    "from torchvision.models.resnet import ResNet18_Weights\n",
    "from torch.utils.data import Dataset, DataLoader,Subset,ConcatDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 数据预处理和加载\n",
    "transform = transforms.Compose(transforms=[\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.transforms.RandomHorizontalFlip(p=0.2), \n",
    "    transforms.RandomRotation(degrees=20),  \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "test_transform = transforms.Compose(transforms=[\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "# 定义 Flower102 合成数据集\n",
    "class Flower102SyntheticDataset(Dataset):\n",
    "    def __init__(self, npz_file, transform=transform):\n",
    "        # 加载 npz 文件\n",
    "        data = np.load(npz_file, allow_pickle=True)\n",
    "        self.images = data['arr_0']  # Flower 图片\n",
    "        self.labels = data['arr_1']  # 对应的标签\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.fromarray(self.images[idx].astype('uint8'))  # 转换为 PIL 图像\n",
    "        label = self.labels[idx]  # 取对应的标签\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "class DuplicatedDataset(Dataset):\n",
    "    def __init__(self, original_dataset, num_copies=2):\n",
    "        self.original_dataset = original_dataset\n",
    "        self.num_copies = num_copies\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.original_dataset) * self.num_copies\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.original_dataset[idx % len(self.original_dataset)]\n",
    "\n",
    "\n",
    "npz_file=\"./synthetic_data/UNet_flowers-250-sampling_steps-50000_images-class_condn_True.npz\"\n",
    "\n",
    "def load_sampled_synthetic_data(npz_file, original_size=1020, batch_size=64,scale=2):\n",
    "    dataset = Flower102SyntheticDataset(npz_file)\n",
    "    sample_size = original_size*scale\n",
    "    # 从合成数据集中随机采样\n",
    "    sampled_indices = random.sample(range(len(dataset)), sample_size)\n",
    "    sampled_dataset = Subset(dataset, sampled_indices)\n",
    "\n",
    "    # 使用 DataLoader 加载数据\n",
    "    return sampled_dataset\n",
    "\n",
    "# trainset\n",
    "scale_size= 12\n",
    "trainset = datasets.Flowers102(root='./data', split='train', download=True, transform=transform)\n",
    "\n",
    "syn_trainset = load_sampled_synthetic_data(npz_file, scale=scale_size-1)\n",
    "combined_dataset = ConcatDataset([trainset, syn_trainset])\n",
    "print(f\"syn_data_size:{len(combined_dataset)}\")\n",
    "trainset_2 = DuplicatedDataset(trainset, num_copies=scale_size)\n",
    "print(f\"train_data_size:{len(trainset_2)}\")\n",
    "\n",
    "# valset and testset\n",
    "valset = datasets.Flowers102(root='./data', split='val', download=True, transform=transform)\n",
    "testset = datasets.Flowers102(root='./data', split='test', download=True, transform=test_transform)\n",
    "\n",
    "# data loader\n",
    "num_workers=4\n",
    "batch_size=64\n",
    "trainloader_syn= DataLoader(combined_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "trainloader_2 = DataLoader(trainset_2, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "valloader = DataLoader(valset, batch_size=128, shuffle=False, num_workers=num_workers)\n",
    "testloader = DataLoader(testset, batch_size=128, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "\n",
    "# 创建保存模型的目录\n",
    "os.makedirs('checkpoints', exist_ok=True)\n",
    "\n",
    "dataset_name = 'flowers102'\n",
    "log_dir = f'/root/tf-logs/{dataset_name}'\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "# 训练函数\n",
    "def train_model(model, criterion, optimizer, scheduler, trainloader, valloader, device,writer=writer, num_epochs=5):\n",
    "    global best_val_loss, patience_counter\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        batch_iterator = tqdm(trainloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n",
    "        for inputs, labels in batch_iterator:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            batch_iterator.set_postfix(loss=loss.item())\n",
    "        epoch_loss = running_loss / len(trainloader)\n",
    "        train_accuracy = 100 * correct / total\n",
    "        train_losses.append(epoch_loss)\n",
    "        writer.add_scalar('Loss/train', epoch_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/train', train_accuracy, epoch)\n",
    "        writer.add_scalar('Learning_Rate', scheduler.get_last_lr()[0], epoch)\n",
    "        print(f\"Epoch {epoch+1} Loss: {epoch_loss:.4f} Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "        # 验证模型\n",
    "        val_loss, val_accuracy = evaluate_model(model, criterion, valloader, device,writer, epoch)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "# 评估函数\n",
    "def evaluate_model(model, criterion, dataloader, device, writer,epoch):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc=\">>> Evaluating\", unit=\"batch\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    accuracy = 100 * correct / total\n",
    "    writer.add_scalar('Loss/val' if dataloader == valloader else 'Loss/test', avg_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/val' if dataloader == valloader else 'Accuracy/test', accuracy, epoch)\n",
    "    print(f\"    Accuracy: {accuracy:.2f}% Avg_Loss: {avg_loss:.4f}\")\n",
    "    return avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 192/192 [00:13<00:00, 14.57batch/s, loss=4.23]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 4.6001 Accuracy: 3.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ">>> Evaluating: 100%|██████████| 8/8 [00:01<00:00,  5.36batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Accuracy: 10.29% Avg_Loss: 4.3047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 192/192 [00:13<00:00, 14.70batch/s, loss=0.423]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 1.6525 Accuracy: 61.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ">>> Evaluating: 100%|██████████| 8/8 [00:01<00:00,  5.47batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Accuracy: 58.43% Avg_Loss: 1.8262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 192/192 [00:13<00:00, 14.57batch/s, loss=0.00363]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 0.0815 Accuracy: 97.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ">>> Evaluating: 100%|██████████| 8/8 [00:01<00:00,  5.59batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Accuracy: 72.16% Avg_Loss: 1.2544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 192/192 [00:13<00:00, 14.76batch/s, loss=0.00792]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 0.0174 Accuracy: 99.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ">>> Evaluating: 100%|██████████| 8/8 [00:01<00:00,  5.31batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Accuracy: 74.51% Avg_Loss: 1.1873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 192/192 [00:13<00:00, 14.70batch/s, loss=0.000428]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 0.0016 Accuracy: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ">>> Evaluating: 100%|██████████| 8/8 [00:01<00:00,  5.59batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Accuracy: 75.88% Avg_Loss: 1.2024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 192/192 [00:13<00:00, 14.56batch/s, loss=0.00242] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Loss: 0.0010 Accuracy: 99.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ">>> Evaluating: 100%|██████████| 8/8 [00:01<00:00,  5.39batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Accuracy: 75.78% Avg_Loss: 1.2987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 192/192 [00:13<00:00, 14.30batch/s, loss=0.000383]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Loss: 0.0005 Accuracy: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ">>> Evaluating: 100%|██████████| 8/8 [00:01<00:00,  5.48batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Accuracy: 75.88% Avg_Loss: 1.2530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 192/192 [00:13<00:00, 14.63batch/s, loss=0.00019] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Loss: 0.0003 Accuracy: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ">>> Evaluating: 100%|██████████| 8/8 [00:01<00:00,  5.56batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Accuracy: 76.18% Avg_Loss: 1.2941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 192/192 [00:13<00:00, 14.58batch/s, loss=0.000193]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Loss: 0.0003 Accuracy: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ">>> Evaluating: 100%|██████████| 8/8 [00:01<00:00,  5.49batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Accuracy: 75.69% Avg_Loss: 1.2873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 192/192 [00:13<00:00, 14.76batch/s, loss=0.000475]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Loss: 0.0003 Accuracy: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ">>> Evaluating: 100%|██████████| 8/8 [00:01<00:00,  5.45batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Accuracy: 74.31% Avg_Loss: 1.3247\n",
      "--->\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ">>> Evaluating: 100%|██████████| 49/49 [00:06<00:00,  7.57batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Accuracy: 73.05% Avg_Loss: 1.5811\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.5810608550221945, 73.05252886648235)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "# 初始化模型 预训练模型\n",
    "model = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 102)\n",
    "model = model.to(device)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "# 定义 OneCycleLR 学习率调度器\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001, steps_per_epoch=len(trainloader_2), epochs=num_epochs)\n",
    "\n",
    "# 训练和评估模型\n",
    "train_model(model, criterion, optimizer, scheduler, trainloader_2, valloader, device, writer,num_epochs)\n",
    "\n",
    "# 关闭 TensorBoard\n",
    "writer.close()\n",
    "\n",
    "print(f\"--->\")\n",
    "evaluate_model(model, criterion, testloader, device, writer,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 192/192 [00:08<00:00, 22.63batch/s, loss=4.56]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 4.6923 Accuracy: 1.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ">>> Evaluating: 100%|██████████| 8/8 [00:01<00:00,  5.22batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Accuracy: 6.08% Avg_Loss: 4.4504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 192/192 [00:08<00:00, 23.28batch/s, loss=1.78]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 3.3611 Accuracy: 23.29%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ">>> Evaluating: 100%|██████████| 8/8 [00:01<00:00,  5.38batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Accuracy: 56.27% Avg_Loss: 1.6783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 192/192 [00:08<00:00, 23.05batch/s, loss=1.48] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 1.5853 Accuracy: 59.51%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ">>> Evaluating: 100%|██████████| 8/8 [00:01<00:00,  5.30batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Accuracy: 68.43% Avg_Loss: 1.1476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 192/192 [00:08<00:00, 23.05batch/s, loss=0.995]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 1.0601 Accuracy: 71.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ">>> Evaluating: 100%|██████████| 8/8 [00:01<00:00,  5.39batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Accuracy: 78.04% Avg_Loss: 0.7656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 192/192 [00:08<00:00, 22.61batch/s, loss=0.666]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 0.7693 Accuracy: 78.43%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ">>> Evaluating: 100%|██████████| 8/8 [00:01<00:00,  5.24batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Accuracy: 86.08% Avg_Loss: 0.4943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 192/192 [00:08<00:00, 22.94batch/s, loss=0.0727]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Loss: 0.5484 Accuracy: 84.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ">>> Evaluating: 100%|██████████| 8/8 [00:01<00:00,  5.42batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Accuracy: 84.41% Avg_Loss: 0.5639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 192/192 [00:08<00:00, 23.11batch/s, loss=0.426]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Loss: 0.4119 Accuracy: 88.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ">>> Evaluating: 100%|██████████| 8/8 [00:01<00:00,  5.23batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Accuracy: 85.10% Avg_Loss: 0.5720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 192/192 [00:08<00:00, 22.83batch/s, loss=0.337] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Loss: 0.2787 Accuracy: 91.81%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ">>> Evaluating: 100%|██████████| 8/8 [00:01<00:00,  5.35batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Accuracy: 87.45% Avg_Loss: 0.4414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 192/192 [00:08<00:00, 23.08batch/s, loss=0.294] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Loss: 0.1711 Accuracy: 95.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ">>> Evaluating: 100%|██████████| 8/8 [00:01<00:00,  5.32batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Accuracy: 88.82% Avg_Loss: 0.4093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 192/192 [00:08<00:00, 23.12batch/s, loss=0.069] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Loss: 0.1049 Accuracy: 97.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ">>> Evaluating: 100%|██████████| 8/8 [00:01<00:00,  5.33batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Accuracy: 88.92% Avg_Loss: 0.3969\n",
      "--->\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ">>> Evaluating: 100%|██████████| 49/49 [00:06<00:00,  7.49batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Accuracy: 77.70% Avg_Loss: 1.1579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.1578952986366895, 77.70369165718003)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "num_epochs = 10\n",
    "dataset_name = 'flowers102_synthetic'\n",
    "log_dir = f'/root/tf-logs/{dataset_name}'\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, trainloader, valloader, device,writer, num_epochs=5):\n",
    "\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        batch_iterator = tqdm(trainloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n",
    "        for inputs, labels in batch_iterator:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            batch_iterator.set_postfix(loss=loss.item())\n",
    "        epoch_loss = running_loss / len(trainloader)\n",
    "        train_accuracy = 100 * correct / total\n",
    "        train_losses.append(epoch_loss)\n",
    "        writer.add_scalar('Loss/train', epoch_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/train', train_accuracy, epoch)\n",
    "        writer.add_scalar('Learning_Rate', scheduler.get_last_lr()[0], epoch)\n",
    "        print(f\"Epoch {epoch+1} Loss: {epoch_loss:.4f} Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "        # 验证模型\n",
    "        val_loss, val_accuracy = evaluate_model(model, criterion, valloader, device,writer, epoch)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "# 评估函数\n",
    "def evaluate_model(model, criterion, dataloader, device, writer,epoch):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc=\">>> Evaluating\", unit=\"batch\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    accuracy = 100 * correct / total\n",
    "    writer.add_scalar('Loss/val' if dataloader == valloader else 'Loss/test', avg_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/val' if dataloader == valloader else 'Accuracy/test', accuracy, epoch)\n",
    "    print(f\"    Accuracy: {accuracy:.2f}% Avg_Loss: {avg_loss:.4f}\")\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# 初始化模型 预训练模型\n",
    "model_syn = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "num_ftrs = model_syn.fc.in_features\n",
    "model_syn.fc = nn.Linear(num_ftrs, 102)\n",
    "model_syn = model_syn.to(device)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_syn = optim.SGD(model_syn.parameters(), lr=0.001, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "# 定义 OneCycleLR 学习率调度器\n",
    "\n",
    "scheduler_syn = torch.optim.lr_scheduler.OneCycleLR(optimizer_syn, max_lr=0.001, steps_per_epoch=len(trainloader_syn), epochs=num_epochs)\n",
    "\n",
    "# 训练和评估模型\n",
    "train_model(model_syn, criterion, optimizer_syn, scheduler_syn, trainloader_syn, valloader, device, writer,num_epochs)\n",
    "\n",
    "# 关闭 TensorBoard\n",
    "writer.close()\n",
    "\n",
    "print(f\"--->\")\n",
    "evaluate_model(model_syn, criterion, testloader, device, writer,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import umap\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# def plot_umap_with_labels(features, labels, class_names, n_neighbors=15, min_dist=0.1, n_components=2):\n",
    "#     features = StandardScaler().fit_transform(features)\n",
    "#     reducer = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components)\n",
    "#     embedding = reducer.fit_transform(features)\n",
    "    \n",
    "#     plt.figure(figsize=(20, 16))  # 增大图像尺寸以适应更多标签\n",
    "#     scatter = plt.scatter(embedding[:, 0], embedding[:, 1], c=labels, cmap='Spectral', s=10)\n",
    "    \n",
    "#     # 使用文本而不是颜色条来表示类别\n",
    "#     for i, class_name in enumerate(class_names):\n",
    "#         idx = labels == i\n",
    "#         if np.any(idx):\n",
    "#             centroid = np.mean(embedding[idx], axis=0)\n",
    "#             plt.annotate(class_name, centroid, fontsize=8, alpha=0.7)\n",
    "    \n",
    "#     plt.title(\"UMAP projection of Flowers102 dataset\")\n",
    "#     plt.xlabel('UMAP Dimension 1')\n",
    "#     plt.ylabel('UMAP Dimension 2')\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# def extract_features(model, dataloader, device):\n",
    "#     model.eval()\n",
    "#     features = []\n",
    "#     labels = []\n",
    "#     with torch.no_grad():\n",
    "#         for inputs, targets in tqdm(dataloader, desc=\"Extracting Features\"):\n",
    "#             inputs = inputs.to(device)\n",
    "#             outputs = model(inputs)\n",
    "#             features.append(outputs.cpu().numpy())\n",
    "#             labels.append(targets.numpy())\n",
    "#     return np.vstack(features), np.concatenate(labels)\n",
    "\n",
    "# # 运行特征提取\n",
    "# features, labels = extract_features(model_syn, testloader, device)\n",
    "\n",
    "# # 加载花名（假设您已经下载了包含花名的文本文件）\n",
    "# class_names = class_names = [\n",
    "#     'pink primrose', 'hard-leaved pocket orchid', 'canterbury bells', 'sweet pea',\n",
    "#     'english marigold', 'tiger lily', 'moon orchid', 'bird of paradise', 'monkshood',\n",
    "#     'globe thistle', 'snapdragon', \"colt's foot\", 'king protea', 'spear thistle',\n",
    "#     'yellow iris', 'globe-flower', 'purple coneflower', 'peruvian lily', 'balloon flower',\n",
    "#     'giant white arum lily', 'fire lily', 'pincushion flower', 'fritillary',\n",
    "#     'red ginger', 'grape hyacinth', 'corn poppy', 'prince of wales feathers',\n",
    "#     'stemless gentian', 'artichoke', 'sweet william', 'carnation',\n",
    "#     'garden phlox', 'love in the mist', 'mexican aster', 'alpine sea holly',\n",
    "#     'ruby-lipped cattleya', 'cape flower', 'great masterwort', 'siam tulip',\n",
    "#     'lenten rose', 'barbeton daisy', 'daffodil', 'sword lily', 'poinsettia',\n",
    "#     'bolero deep blue', 'wallflower', 'marigold', 'buttercup', 'oxeye daisy',\n",
    "#     'common dandelion', 'petunia', 'wild pansy', 'primula', 'sunflower',\n",
    "#     'pelargonium', 'bishop of llandaff', 'gaura', 'geranium', 'orange dahlia',\n",
    "#     'pink-yellow dahlia?', 'cautleya spicata', 'japanese anemone', 'black-eyed susan',\n",
    "#     'silverbush', 'californian poppy', 'osteospermum', 'spring crocus',\n",
    "#     'bearded iris', 'windflower', 'tree poppy', 'gazania', 'azalea', 'water lily',\n",
    "#     'rose', 'thorn apple', 'morning glory', 'passion flower', 'lotus', 'toad lily',\n",
    "#     'anthurium', 'frangipani', 'clematis', 'hibiscus', 'columbine', 'desert-rose',\n",
    "#     'tree mallow', 'magnolia', 'cyclamen', 'watercress', 'canna lily', 'hippeastrum',\n",
    "#     'bee balm', 'ball moss', 'foxglove', 'bougainvillea', 'camellia', 'mallow',\n",
    "#     'mexican petunia', 'bromelia', 'blanket flower', 'trumpet creeper',\n",
    "#     'blackberry lily'\n",
    "# ]\n",
    "\n",
    "# # 运行 UMAP 并显示\n",
    "# plot_umap_with_labels(features, labels, class_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
